# -*- coding: utf-8 -*-
"""Project Sprint 12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tBXnCkj_R8QHQqK3iPs32O8l8oHaA0dY

# **Car Value Prediction Model**

## **Introduction**

Rusty Bargain (not a real company) is a used car trading company that is developing an application to attract new buyers. This application will allow users to quickly determine the market value of their cars. Users will have access to historical data, vehicle technical specifications, model versions, and car prices. The goal of this project is to develop a model capable of determining the market value of cars.

### **Goals**

Rusty Bargain is particularly interested in:
1. **Prediction quality**
2. **Prediction speed**
3. **Training time of the model**

### **Steps**

1. **Data Observation and Storage**: Begin by saving and observing the provided dataset.
2. **Model Training**: Train different models with various hyperparameters. At least two distinct models should be created, but more are preferable. Note that different implementations of gradient boosting do not count as separate models. The primary objective of this step is to compare gradient boosting methods with random forest, decision tree, and linear regression.
3. **Model Analysis**: Analyze the speed and quality of each model.

### **Data Description**

#### Features
- **DateCrawled**: The date when the profile was downloaded from the database
- **VehicleType**: The body type of the vehicle
- **RegistrationYear**: The year the vehicle was registered
- **Gearbox**: The type of transmission
- **Power**: Power in horsepower (hp)
- **Model**: The model of the vehicle
- **Mileage**: Distance traveled (measured in km according to a specific regional dataset)
- **RegistrationMonth**: The month the vehicle was registered
- **FuelType**: The type of fuel used
- **Brand**: The brand of the vehicle
- **NotRepaired**: Indicates if the vehicle has been previously repaired
- **DateCreated**: The date the profile was created
- **NumberOfPictures**: The number of pictures of the vehicle
- **PostalCode**: The postal code of the profile owner (user)
- **LastSeen**: The date of the user's last activity

#### Target
- **Price**: The price of the vehicle (in Euros)

## **Data Loadment & Exploratory**
"""

!pip install catboost

# Libraries for data processing
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time
from sklearn.model_selection import train_test_split

# Import Library for Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error
import lightgbm as lgb
from catboost import CatBoostRegressor
import xgboost as xg

# Import Library for model evaluation
from sklearn.metrics import confusion_matrix, roc_auc_score

"""### **Data Loadment**"""

#load data
path = '/content/car_data.csv'

df = pd.read_csv(path)
df.head(3)

"""### **Data Checking**"""

# Understand the number of columns and rows

df.shape

# Understand the general information regarding the columns content

df.info()

# Check missing value

df.isnull().sum()

# Check duplicated value

df.duplicated().sum()

df.drop_duplicates(inplace=True)
df.info()

"""### **Exploratory Data Analysis**"""

# Check the correlation
# Select relevant columns for correlation
columns_of_interest = ['Price', 'RegistrationYear', 'Power', 'Mileage', 'RegistrationMonth', 'NumberOfPictures', 'PostalCode']
df_selected = df[columns_of_interest]

# Calculate the correlation matrix
correlation_matrix = df_selected.corr()

# Display the correlation matrix
correlation_matrix

df_eda = df.copy()
df_eda.head(2)

df_eda['VehicleType'].unique()

df_eda_1 = df_eda.pivot_table(index='VehicleType', values='Price', aggfunc='count', dropna=False).reset_index()
df_eda_1.columns = ['VehicleType', 'Qty']
df_eda_1 = df_eda_1.sort_values(by='Qty', ascending=False)
df_eda_1

sns.barplot(data= df_eda_1, x='VehicleType', y='Qty')
plt.show()

df_eda_2 = df_eda.pivot_table(index='FuelType', values='Price', aggfunc='count', dropna=False).reset_index()
df_eda_2.columns = ['FuelType', 'Qty']
df_eda_2 = df_eda_2.sort_values(by='Qty', ascending=False)
df_eda_2

sns.barplot(data= df_eda_2, x='FuelType', y='Qty')
plt.show()

df_eda_3 = df_eda.pivot_table(index='Model', values='Price', aggfunc='count').reset_index()
df_eda_3.columns = ['Model', 'Qty']
df_eda_3 = df_eda_3.sort_values(by='Qty', ascending=False).head(10)
df_eda_3

sns.barplot(data= df_eda_3, x='Model', y='Qty')
plt.show()

df_eda_4 = df_eda.pivot_table(index='Gearbox', values='Price', aggfunc='count', dropna=False).reset_index()
df_eda_4.columns = ['Gearbox', 'Qty']
df_eda_4 = df_eda_4.sort_values(by='Qty', ascending=False).head(10)
df_eda_4

sns.barplot(data= df_eda_4, x='Gearbox', y='Qty')
plt.show()

df_eda_5 = df_eda.pivot_table(index='NotRepaired', values='Price', aggfunc='count').reset_index()
df_eda_5.columns = ['NotRepaired', 'Qty']
df_eda_5 = df_eda_5.sort_values(by='Qty', ascending=False).head(10)
df_eda_5

sns.barplot(data= df_eda_5, x='NotRepaired', y='Qty')
plt.show()

# Drop the missing value in dataset
df = df.dropna(subset=['VehicleType', 'Gearbox', 'Model', 'FuelType', 'NotRepaired'])
df.isnull().sum()

"""### **Summary**

1. The data has 27077 rows and 16 columns.
2. Missing value was found in columns VehicleType with 2933 rows, Gearbox with 1505 rows, Model with 1474 rows, FuelType with 2570 rows, and NotRepaired with 5453 rows, and all of the missing value has been removed.
3. 1 duplicated data was found and has been removed.
4. Top 3 vehicle types are sedan, small, and wagon.
5. Top 3 fuel types are petrol, gasoline, and lpg.
6. Top 3 model types are golf, other type of vehicle, and 3er.

## **Developping Machine Learning Model**
"""

# OHE
column_to_del = ['DateCrawled', 'DateCreated', 'LastSeen', 'PostalCode', 'NumberOfPictures', 'RegistrationMonth']
df = df.drop(column_to_del, axis=1)

df_ohe = pd.get_dummies(df)
df_ohe = df_ohe.sample(15000)
df_ohe.shape

# Split the data into train dataset and test dataset
df_ohe_train_valid, df_ohe_test = train_test_split(df_ohe, test_size=0.15, random_state=12345)
df_ohe_train, df_ohe_valid = train_test_split(df_ohe_train_valid, test_size=0.25, random_state=12345)

print(df_ohe_train.shape)
print(df_ohe_test.shape)
print(df_ohe_valid.shape)

# RMSE Calculation function
def rmse(y, predictions):
    return mean_squared_error(y, predictions)**0.5

# State the target and features
X_train = df_ohe_train.drop(['Price'], axis=1)
y_train = df_ohe_train['Price']

X_test = df_ohe_test.drop(['Price'], axis=1)
y_test = df_ohe_test['Price']

X_valid = df_ohe_valid.drop(['Price'], axis=1)
y_valid = df_ohe_valid['Price']

print('Train dataset')
print(X_train.shape)
print(y_train.shape)
print('Test dataset')
print(X_test.shape)
print(y_test.shape)
print('Valid dataset')
print(X_valid.shape)
print(y_valid.shape)

"""### **Logistic Regression**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Create logistic regression model
# lr = LogisticRegression(max_iter=200)
# # Training Model
# lr.fit(X_train, y_train)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Training prediction
# X_train_pred_lr = lr.predict(X_train)
# X_test_pred_lr = lr.predict(X_test)
# X_valid_pred_lr = lr.predict(X_valid)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Calculate RMSE values and round them to 3 decimal places
# rmse_train_lr = rmse(y_train, X_train_pred_lr).round(3)
# rmse_test_lr = rmse(y_test, X_test_pred_lr).round(3)
# rmse_valid_lr = rmse(y_valid, X_valid_pred_lr).round(3)
# 
# # Store the wall time manually from the %%time output after running the cell
# wall_time = (3 * 60) + 21
# 
# # Create a DataFrame to display the RMSE and prediction time results
# results_lr = pd.DataFrame({
#     'model': ['Logistic Regression'],
#     'rmse_train': [rmse_train_lr],
#     'rmse_test': [rmse_test_lr],
#     'rmse_valid': [rmse_valid_lr],
#     'wall_time_sec': [wall_time]
# })
# 
# # Print the DataFrame
# results_lr

"""### **Random Forrest Classifier**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Create random forrest classifier model
# rf = RandomForestClassifier()
# # Training Model
# rf.fit(X_train, y_train)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Training prediction
# X_train_pred_rf = rf.predict(X_train)
# X_test_pred_rf = rf.predict(X_test)
# X_valid_pred_rf = rf.predict(X_valid)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Calculate RMSE values and round them to 3 decimal places
# rmse_train_rf = rmse(y_train, X_train_pred_rf).round(3)
# rmse_test_rf = rmse(y_test, X_test_pred_rf).round(3)
# rmse_valid_rf = rmse(y_valid, X_valid_pred_rf).round(3)
# 
# # Store the wall time manually from the %%time output after running the cell
# wall_time = 28
# 
# # Create a DataFrame to display the RMSE and prediction time results
# results_rf = pd.DataFrame({
#     'model': ['Random Forrest Classifier'],
#     'rmse_train': [rmse_train_rf],
#     'rmse_test': [rmse_test_rf],
#     'rmse_valid': [rmse_valid_rf],
#     'wall_time_sec': [wall_time]
# })
# 
# # Print the DataFrame
# results_rf

"""### **Light Gradient Boosting**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Create light gradient boosting model
# lg = lgb.LGBMRegressor(num_iterations=20, verbose=0, metric='rmse')
# # Training Model
# lg.fit(X_train, y_train, eval_set=(X_valid, y_valid))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Training prediction
# X_train_pred_lg = lg.predict(X_train)
# X_test_pred_lg = lg.predict(X_test)
# X_valid_pred_lg = lg.predict(X_valid)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Calculate RMSE values and round them to 3 decimal places
# rmse_train_xg = rmse(y_train, X_train_pred_lg).round(3)
# rmse_test_xg = rmse(y_test, X_test_pred_lg).round(3)
# rmse_valid_xg = rmse(y_valid, X_valid_pred_lg).round(3)
# 
# # Store the wall time manually from the %%time output after running the cell
# wall_time = 1.1
# 
# # Create a DataFrame to display the RMSE and prediction time results
# results_lg = pd.DataFrame({
#     'model': ['Light Gradient Boosting'],
#     'rmse_train': [rmse_train_xg],
#     'rmse_test': [rmse_test_xg],
#     'rmse_valid': [rmse_valid_xg],
#     'wall_time_sec': [wall_time]
# })
# 
# # Print the DataFrame
# results_lg

"""### **Cat Boost Regressor**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Create Cat Boost Regressor model
# cb = CatBoostRegressor(iterations=20, learning_rate=1, depth=2)
# # Training Model
# cb.fit(X_train, y_train)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Training prediction
# X_train_pred_cb = cb.predict(X_train)
# X_test_pred_cb = cb.predict(X_test)
# X_valid_pred_cb = cb.predict(X_valid)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Calculate RMSE values and round them to 3 decimal places
# rmse_train_cb = rmse(y_train, X_train_pred_cb).round(3)
# rmse_test_cb = rmse(y_test, X_test_pred_cb).round(3)
# rmse_valid_cb = rmse(y_valid, X_valid_pred_cb).round(3)
# 
# # Store the wall time manually from the %%time output after running the cell
# wall_time = 1.5
# 
# # Create a DataFrame to display the RMSE and prediction time results
# results_cb = pd.DataFrame({
#     'model': ['Cat Boost Regressor'],
#     'rmse_train': [rmse_train_cb],
#     'rmse_test': [rmse_test_cb],
#     'rmse_valid': [rmse_valid_cb],
#     'wall_time_sec': [wall_time]
# })
# 
# # Print the DataFrame
# results_cb

"""### **XG Boost Regressor**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Create XG Boost Regressor model
# xg = xg.XGBRegressor(objective ='reg:linear', n_estimators = 10, seed = 123)
# # Training Model
# xg.fit(X_train, y_train)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Training prediction
# X_train_pred_xg = xg.predict(X_train)
# X_test_pred_xg = xg.predict(X_test)
# X_valid_pred_xg = xg.predict(X_valid)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Calculate RMSE values and round them to 3 decimal places
# rmse_train_xg = rmse(y_train, X_train_pred_xg).round(3)
# rmse_test_xg = rmse(y_test, X_test_pred_xg).round(3)
# rmse_valid_xg = rmse(y_valid, X_valid_pred_xg).round(3)
# 
# # Store the wall time manually from the %%time output after running the cell
# wall_time = 5.6
# 
# # Create a DataFrame to display the RMSE and prediction time results
# results_xg = pd.DataFrame({
#     'model': ['XG Boost Regressor'],
#     'rmse_train': [rmse_train_xg],
#     'rmse_test': [rmse_test_xg],
#     'rmse_valid': [rmse_valid_xg],
#     'wall_time_sec': [wall_time]
# })
# 
# # Print the DataFrame
# results_xg

"""### **Model Performance Recapitulation**"""

result_all = pd.concat([results_lr, results_rf, results_lg, results_cb, results_xg], ignore_index=True)
result_all = result_all.sort_values(by='rmse_valid', ascending=True)
result_all

"""## **Conclusion**

## Goals
Aims to develop an application for users to quickly determine the market value of their cars. The primary goals for Rusty Bargain in this project were:

- High prediction quality
- Fast prediction speed
- Efficient model training time

## Data Description
The dataset used for this project contains 27,077 rows and 16 columns.

## Data Cleaning
1. Missing values were identified and removed in the following columns:
- VehicleType: 2,933 rows
- Gearbox: 1,505 rows
- Model: 1,474 rows
- FuelType: 2,570 rows
- NotRepaired: 5,453 rows

2. 1 duplicate row was removed.

**Insight**

1. The top three vehicle types were sedan, small, and wagon.  
2. The top three fuel types were petrol, gasoline, and LPG.  
3. The top three model types were golf, other types of vehicles, and 3er.

## Model Training and Analysis
Five models were trained and compared on their prediction quality, speed, and training time:

| Model                    | RMSE (Train) | RMSE (Test) | RMSE (Validation) | Wall Time (sec) |
|--------------------------|--------------|-------------|-------------------|-----------------|
| Logistic Regression      | 5843.437     | 5979.321    | 5746.115          | 201.0           |
| Random Forest Classifier | 655.695      | 2608.961    | 2657.262          | 28.0            |
| Light GBM                | 2046.961     | 2240.519    | 2181.612          | 1.1             |
| CatBoost Regressor       | 2099.117     | 2286.783    | 2170.087          | 1.5             |
| XGBoost Regressor        | 1716.396     | 2066.720    | 1981.200          | 5.6             |

## Conclusion
Based on the evaluation metrics, the XGBoost Regressor emerged as the best model with the following characteristics:

- **Highest Prediction Quality**: Achieved the lowest RMSE on the test (2066.720) and validation (1981.200) sets, indicating superior accuracy.
- **Reasonable Prediction Speed**: Took 5.6 seconds for wall time, balancing speed and efficiency.
- **Balanced Training Time**: Though not the fastest, the training time was efficient given its high prediction quality.

### Recommendations
- **Implementation**: The XGBoost Regressor should be implemented in the application due to its high accuracy and reasonable performance.
- **Future Improvements**: Consider further tuning of hyperparameters and experimenting with additional data preprocessing techniques to enhance model performance.
- **User Experience**: Ensure the application leverages the model's capabilities to provide quick and accurate car value predictions to attract and retain users.
"""

